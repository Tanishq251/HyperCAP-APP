{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":10760279,"datasetId":6674581,"databundleVersionId":11114338},{"sourceType":"datasetVersion","sourceId":10705285,"datasetId":6426526,"databundleVersionId":11053246},{"sourceType":"datasetVersion","sourceId":10760431,"datasetId":6667013,"databundleVersionId":11114498}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install torchinfo","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"At8tJnXyQHxs","outputId":"3d2befb7-d305-4ecc-b731-75bf08a63c0f","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T01:05:57.899654Z","iopub.execute_input":"2025-02-16T01:05:57.899979Z","iopub.status.idle":"2025-02-16T01:06:02.009477Z","shell.execute_reply.started":"2025-02-16T01:05:57.899955Z","shell.execute_reply":"2025-02-16T01:06:02.008655Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Vision Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\n\nclass CNN_branch(nn.Module):\n    def __init__(self,channels):\n        super().__init__()\n        self.net = nn.Sequential(nn.Conv3d(channels,channels,\n                                           kernel_size=3,padding=1,\n                                           groups=channels),\n                            nn.Conv3d(channels,channels,kernel_size=1),\n                            nn.BatchNorm3d(channels),\n                            nn.ReLU6(),\n                            )\n    def forward(self,x):\n        return x + self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self,heads,patch,drop):\n        super().__init__()\n        self.heads = heads\n        self.patch=patch\n        self.scale = patch**-1\n        self.conv_project = nn.Sequential(nn.Conv3d(1,3*heads,\n                                                    kernel_size=(3,3,1),\n                                                    padding=(1,1,0),\n                                                    bias=False),\n                                          Rearrange('b h x y s -> b s (h x y)'),\n                                          nn.Dropout(drop))\n        self.reduce_k = nn.Conv2d(self.heads,self.heads,\n                                  kernel_size=(3,1),padding=(1,0),stride=(4,1),\n                                  groups=self.heads,bias=False)\n        self.reduce_v = nn.Conv2d(self.heads,self.heads,\n                                  kernel_size=(3,1),padding=(1,0),stride=(4,1),\n                                  groups=self.heads,bias=False)\n        self.conv_out = nn.Sequential(nn.Conv3d(in_channels=heads,\n                                                out_channels=1,\n                                                kernel_size=(3,3,1),\n                                                padding=(1,1,0),bias=False),\n                                      nn.Dropout(drop),\n                                      Rearrange('b c x y s-> b c s x y'),\n                                      nn.LayerNorm((patch,patch)),\n                                      Rearrange('b c s x y->b c x y s')\n                                      )\n    def forward(self,x):\n        qkv = self.conv_project(x).chunk(3,dim=-1)\n        q,k,v = map(lambda a: rearrange(a,'b s (h d) -> b h s d',h=self.heads),\n                    qkv)\n        k = self.reduce_k(k)\n        dots = torch.einsum('bhid,bhjd->bhij',q,k) * self.scale\n        attn = dots.softmax(dim=-1)\n        v = self.reduce_v(v)\n        out = torch.einsum('bhij,bhjd->bhid',attn,v)\n        out = rearrange(out,'b c s (x y) -> b c x y s ',\n                        x=self.patch,y=self.patch)\n        out = self.conv_out(out)\n        return out\n\nclass ConvTE(nn.Module):\n    def __init__(self,heads,patch,drop):\n        super().__init__()\n        self.attention = Attention(heads,patch,drop)\n        self.ffn = nn.Sequential(nn.Conv3d(in_channels=1,out_channels=1,\n                                           kernel_size=(3,3,1),\n                                           padding=(1,1,0),\n                                           bias=False),\n                                 nn.ReLU6(),\n                                 nn.Dropout(drop)\n                                 )\n    def forward(self,x):\n        x = x + self.attention(x)\n        x = x + self.ffn(x)\n        return x\n\nclass DBCT(nn.Module):\n    def __init__(self,channels,patch,heads,drop,fc_dim,band_reduce):\n        super().__init__()\n        self.cnn_branch = CNN_branch(channels)\n        self.convte_branch = nn.Sequential(nn.Conv3d(channels,1,\n                                                     kernel_size=(1,1,7),\n                                                     padding=(0,0,3),\n                                                     stride=(1,1,1)),\n                                           ConvTE(heads,patch,drop)\n                                           )\n        self.cnn_out = nn.Sequential(nn.Conv3d(channels,channels,\n                                                 kernel_size=(3,3,\n                                                              band_reduce),\n                                                 padding=(1,1,0),\n                                                 groups=channels),\n                                       nn.BatchNorm3d(channels),\n                                       nn.ReLU6()\n                                       )\n        self.te_out = nn.Sequential(nn.Conv3d(1,channels,\n                                                  kernel_size=(3,3,\n                                                               band_reduce),\n                                                  padding=(1,1,0)),\n                                        nn.BatchNorm3d(channels),\n                                        nn.ReLU6()\n                                        )\n        self.out = nn.Sequential(nn.Conv3d(2*channels ,fc_dim,kernel_size=1),\n                                nn.BatchNorm3d(fc_dim),\n                                nn.ReLU6()\n                                )\n    def forward(self,x):\n        x_cnn = self.cnn_branch(x)\n        x_te = self.convte_branch(x)\n        cnn_out = self.cnn_out(x_cnn)\n        te_out = self.te_out(x_te)\n        out = self.out(torch.cat((cnn_out,te_out),dim=1))\n        return out\n\nclass MSpeFE(nn.Module):\n    def __init__(self,channels):\n        super().__init__()\n        self.c = channels // 4\n        self.spectral1 = nn.Sequential(nn.Conv3d(self.c,self.c,\n                                                 kernel_size=(1,1,3),\n                                                 padding=(0,0,1),\n                                                 groups=self.c),\n                                                 nn.BatchNorm3d(self.c),\n                                                 nn.ReLU6()\n                                                 )\n        self.spectral2 = nn.Sequential(nn.Conv3d(self.c,self.c,\n                                                 kernel_size=(1,1,7),\n                                                 padding=(0,0,3),\n                                                 groups=self.c),\n                                                 nn.BatchNorm3d(self.c),\n                                                 nn.ReLU6()\n                                                 )\n        self.spectral3 = nn.Sequential(nn.Conv3d(self.c,self.c,\n                                                 kernel_size=(1,1,11),\n                                                 padding=(0,0,5),\n                                                 groups=self.c),\n                                                 nn.BatchNorm3d(self.c),\n                                                 nn.ReLU6()\n                                                 )\n        self.spectral4 = nn.Sequential(nn.Conv3d(self.c,self.c,\n                                                 kernel_size=(1,1,15),\n                                                 padding=(0,0,7),\n                                                 groups=self.c),\n                                                 nn.BatchNorm3d(self.c),\n                                                 nn.ReLU6()\n                                                 )\n\n    def forward(self,x):\n        x1 = self.spectral1(x[:,0:self.c,:])\n        x2 = self.spectral2(x[:,self.c:2*self.c,:])\n        x3 = self.spectral3(x[:,2*self.c:3*self.c,:])\n        x4 = self.spectral4(x[:,3*self.c:,:])\n        mspe = torch.cat((x1,x2,x3,x4),dim=1)\n        return mspe\n\nclass DBCTNet(nn.Module):\n    def __init__(self,channels=16,patch=9,bands=270,\n                 fc_dim=16,heads=2,drop=0.1):\n        super().__init__()\n        self.band_reduce = (bands - 7) // 2 + 1\n        self.stem = nn.Conv3d(1,channels,kernel_size=(1,1,7),\n                                            padding=0,stride=(1,1,2))\n        self.mspefe = MSpeFE(channels)\n\n        self.dbct = DBCT(channels,patch,heads,drop,fc_dim,self.band_reduce)\n\n        # self.fc = nn.Sequential(nn.AdaptiveAvgPool3d((1,1,1)),\n        #                         nn.Flatten(),\n        #                         nn.Linear(fc_dim, num_class)\n        #                         )\n\n    def forward(self,x):\n        # x.shape = [batch_size,1,patch_size,patch_size,spectral_bands]\n        b,_,_,_,_ = x.shape\n        x = self.stem(x)\n        x = self.mspefe(x)\n        feature = self.dbct(x)\n        return feature\n        # return self.fc(feature)\n\nimport torch\nfrom torchinfo import summary\n\n# Define the model\nmodel = DBCTNet(bands=200)\ndevice = torch.device(\"cuda:0\")\nmodel = model.to(device)\nmodel.eval()\n\n# Define input tensor\ninput_tensor = torch.randn(4, 1, 9, 9, 200).cuda()\n\n# Print model summary\nsummary(model, input_size=(4, 1, 9, 9, 200), col_names=[\"input_size\", \"output_size\", \"num_params\"])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lkcUfCW7PdTP","outputId":"db821e80-40f6-4a11-99a9-b502aebccca0","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T01:06:02.010771Z","iopub.execute_input":"2025-02-16T01:06:02.011041Z","iopub.status.idle":"2025-02-16T01:06:02.834573Z","shell.execute_reply.started":"2025-02-16T01:06:02.011018Z","shell.execute_reply":"2025-02-16T01:06:02.833837Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"========================================================================================================================\nLayer (type:depth-idx)                        Input Shape               Output Shape              Param #\n========================================================================================================================\nDBCTNet                                       [4, 1, 9, 9, 200]         [4, 16, 9, 9, 1]          --\n├─Conv3d: 1-1                                 [4, 1, 9, 9, 200]         [4, 16, 9, 9, 97]         128\n├─MSpeFE: 1-2                                 [4, 16, 9, 9, 97]         [4, 16, 9, 9, 97]         --\n│    └─Sequential: 2-1                        [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          --\n│    │    └─Conv3d: 3-1                       [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          16\n│    │    └─BatchNorm3d: 3-2                  [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          8\n│    │    └─ReLU6: 3-3                        [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          --\n│    └─Sequential: 2-2                        [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          --\n│    │    └─Conv3d: 3-4                       [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          32\n│    │    └─BatchNorm3d: 3-5                  [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          8\n│    │    └─ReLU6: 3-6                        [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          --\n│    └─Sequential: 2-3                        [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          --\n│    │    └─Conv3d: 3-7                       [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          48\n│    │    └─BatchNorm3d: 3-8                  [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          8\n│    │    └─ReLU6: 3-9                        [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          --\n│    └─Sequential: 2-4                        [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          --\n│    │    └─Conv3d: 3-10                      [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          64\n│    │    └─BatchNorm3d: 3-11                 [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          8\n│    │    └─ReLU6: 3-12                       [4, 4, 9, 9, 97]          [4, 4, 9, 9, 97]          --\n├─DBCT: 1-3                                   [4, 16, 9, 9, 97]         [4, 16, 9, 9, 1]          --\n│    └─CNN_branch: 2-5                        [4, 16, 9, 9, 97]         [4, 16, 9, 9, 97]         --\n│    │    └─Sequential: 3-13                  [4, 16, 9, 9, 97]         [4, 16, 9, 9, 97]         752\n│    └─Sequential: 2-6                        [4, 16, 9, 9, 97]         [4, 1, 9, 9, 97]          --\n│    │    └─Conv3d: 3-14                      [4, 16, 9, 9, 97]         [4, 1, 9, 9, 97]          113\n│    │    └─ConvTE: 3-15                      [4, 1, 9, 9, 97]          [4, 1, 9, 9, 97]          255\n│    └─Sequential: 2-7                        [4, 16, 9, 9, 97]         [4, 16, 9, 9, 1]          --\n│    │    └─Conv3d: 3-16                      [4, 16, 9, 9, 97]         [4, 16, 9, 9, 1]          13,984\n│    │    └─BatchNorm3d: 3-17                 [4, 16, 9, 9, 1]          [4, 16, 9, 9, 1]          32\n│    │    └─ReLU6: 3-18                       [4, 16, 9, 9, 1]          [4, 16, 9, 9, 1]          --\n│    └─Sequential: 2-8                        [4, 1, 9, 9, 97]          [4, 16, 9, 9, 1]          --\n│    │    └─Conv3d: 3-19                      [4, 1, 9, 9, 97]          [4, 16, 9, 9, 1]          13,984\n│    │    └─BatchNorm3d: 3-20                 [4, 16, 9, 9, 1]          [4, 16, 9, 9, 1]          32\n│    │    └─ReLU6: 3-21                       [4, 16, 9, 9, 1]          [4, 16, 9, 9, 1]          --\n│    └─Sequential: 2-9                        [4, 32, 9, 9, 1]          [4, 16, 9, 9, 1]          --\n│    │    └─Conv3d: 3-22                      [4, 32, 9, 9, 1]          [4, 16, 9, 9, 1]          528\n│    │    └─BatchNorm3d: 3-23                 [4, 16, 9, 9, 1]          [4, 16, 9, 9, 1]          32\n│    │    └─ReLU6: 3-24                       [4, 16, 9, 9, 1]          [4, 16, 9, 9, 1]          --\n========================================================================================================================\nTotal params: 30,032\nTrainable params: 30,032\nNon-trainable params: 0\nTotal mult-adds (M): 47.11\n========================================================================================================================\nInput size (MB): 0.26\nForward/backward pass size (MB): 27.16\nParams size (MB): 0.12\nEstimated Total Size (MB): 27.54\n========================================================================================================================"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Text model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertModel, BertTokenizer\n\nVISION_MODEL_INFO = {\n    'DBCTNet': {\n        'out_dim': 1296,\n        'model': DBCTNet,\n    },\n}\n\nclass Model(nn.Module):\n    def __init__(self, num_classes, vision_encoder_name, bands, merging_method, max_length=64):\n        super(Model, self).__init__()\n\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n        self.max_length = max_length\n\n        for param in self.bert.parameters():\n            param.requires_grad = False\n\n        self.bert_fc = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n\n        self.vision_encoder = VISION_MODEL_INFO[vision_encoder_name]['model'](bands=bands)\n        vision_features = VISION_MODEL_INFO[vision_encoder_name]['out_dim']\n        self.vision_fc = nn.Linear(vision_features, self.bert.config.hidden_size)\n\n        self.merging_method = merging_method\n\n        if merging_method == 'CONCAT':\n            self.fc = nn.Linear(self.bert.config.hidden_size * 2, num_classes)\n        elif merging_method in ['PWA', 'PWM']:\n            self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def tokenize(self, text):\n        tokens = self.tokenizer(\n            text, padding=\"max_length\", max_length=self.max_length, truncation=True, return_tensors=\"pt\"\n        )\n        return tokens[\"input_ids\"], tokens[\"attention_mask\"]\n\n    def forward_text_features(self, text):\n        input_ids, attention_mask = self.tokenize(text)\n        input_ids, attention_mask = input_ids.to(next(self.parameters()).device), attention_mask.to(next(self.parameters()).device)\n\n        with torch.no_grad():\n            text_out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n\n        text_emb = self.bert_fc(text_out)\n        text_out = text_out + text_emb\n\n        return text_out\n\n    def forward_vision_features(self, image):\n        vision_out = self.vision_encoder(image)\n        vision_out = vision_out.view(vision_out.size(0), -1)\n        vision_out = self.vision_fc(vision_out)\n        return vision_out\n\n    def forward(self, image, text):\n        text_emb = self.forward_text_features(text)\n        vision_emb = self.forward_vision_features(image)\n\n        if self.merging_method == 'CONCAT':\n            cls_output = torch.cat((text_emb, vision_emb), dim=1)\n        elif self.merging_method == 'PWA':\n            cls_output = text_emb + vision_emb\n        elif self.merging_method == 'PWM':\n            cls_output = text_emb * vision_emb\n\n        return self.fc(cls_output)\n\n# Example usage\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_classes = 9\nvision_encoder_name = 'DBCTNet'\nbands = 270\nmerging_method='CONCAT'  # 'PWA', 'PWM'\n\nmodel = Model(\n    num_classes=num_classes,\n    vision_encoder_name=vision_encoder_name,\n    bands=bands,\n    merging_method=merging_method\n    ).to(device)\n\n# Example text input\ntext = \"This is a sample text for classification.\"\nimage = torch.randn(1, 1, 9, 9, 270).cuda()\noutput = model(image, text)\noutput.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nU84W3MwRU6m","outputId":"faccab4d-8b0f-4699-e977-3e8f7be4a3ca","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T01:06:02.836331Z","iopub.execute_input":"2025-02-16T01:06:02.836575Z","iopub.status.idle":"2025-02-16T01:06:07.070657Z","shell.execute_reply.started":"2025-02-16T01:06:02.836538Z","shell.execute_reply":"2025-02-16T01:06:07.069874Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9496767d97f54ef6b201e4f61d7e7507"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"821e20ce4c3640ad8a70238e03796c81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de72eb09764743419c26de7963b62b98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3f2022547364246a1d73d653a58065e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6611dd6187b417fa37838fc6bce7b91"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 9])"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Data processing Pipeline","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score, confusion_matrix, cohen_kappa_score, precision_score, recall_score\nimport numpy as np\nimport random\nimport sys\nsys.path.append(\"/kaggle/input/util-py-1\")\nfrom PPreprocess import DatasetPreprocess\nimport img as util\n\nclass HSITextDataset(Dataset):\n    def __init__(self, hsi_data, labels, text_data, max_length=64):\n        self.hsi_data = hsi_data\n        self.labels = labels\n        self.text_data = text_data\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.hsi_data)\n\n    def __getitem__(self, idx):\n        hsi = self.hsi_data[idx]\n        label = self.labels[idx]\n        text = self.text_data.iloc[idx].tolist()  # Convert Series to list\n        return hsi, text, label\n\n\nfolder_name = '/kaggle/working/Indian Pines'\ndataset_preprocess = DatasetPreprocess(folder_name)\ndata, gt, captions = dataset_preprocess.load_data()\n\n# Split the data\ntrain_fraction = 0.10\nrem_classes = [0]\n(train_rows, train_cols), (test_rows, test_cols) = util.data_split(gt, train_fraction=train_fraction, rem_classes=rem_classes)\n\ntext_csv_path = os.path.join(folder_name, 'Indian_pines.csv')\ntext_df = pd.read_csv(text_csv_path)\n\ntrain_text, val_text = util.split_text_data_based_on_spatial(text_df, gt, train_rows, train_cols)\n\n(train_input_sub, y_train_sub), (val_input, y_val), (test_input, y_test) = dataset_preprocess.get_patchify_data(patch_size=9)\n\ntrain_input_tensor = torch.tensor(train_input_sub, dtype=torch.float32)\nval_input_tensor = torch.tensor(val_input, dtype=torch.float32)\ntrain_labels_tensor = torch.tensor(y_train_sub, dtype=torch.long)\nval_labels_tensor = torch.tensor(y_val, dtype=torch.long)\n\n# Add a new dimension if necessary\nif len(train_input_tensor.shape) == 4:\n    train_input_tensor = train_input_tensor.unsqueeze(1)\nif len(val_input_tensor.shape) == 4:\n    val_input_tensor = val_input_tensor.unsqueeze(1)\n\n# Permute dimensions\nif len(train_input_tensor.shape) == 5:\n    train_input_tensor = train_input_tensor.permute(0, 1, 3, 4, 2)\nif len(val_input_tensor.shape) == 5:\n    val_input_tensor = val_input_tensor.permute(0, 1, 3, 4, 2)\n\ntrain_dataset = HSITextDataset(\n    train_input_tensor,\n    train_labels_tensor,\n    train_text,\n    max_length=64\n)\n\nval_dataset = HSITextDataset(\n    val_input_tensor,\n    val_labels_tensor,\n    val_text,\n    max_length=64\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=True,\n    num_workers=0,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=True\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_classes = 16\nvision_encoder_name = 'DBCTNet'\nbands = 200\nmerging_method = 'CONCAT'\n\nmodel = Model(\n    num_classes=num_classes,\n    vision_encoder_name=vision_encoder_name,\n    bands=bands,\n    merging_method=merging_method\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ndef train_model(model, train_loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n    for hsi, text, labels in progress_bar:\n        hsi, labels = hsi.to(device), labels.to(device)\n        text = [t for t in text]\n        optimizer.zero_grad()\n        outputs = model(hsi, text[1])\n        labels = labels.float()\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        progress_bar.set_postfix({'loss': total_loss / (progress_bar.n + 1)})\n    return total_loss / len(train_loader)\n\ndef evaluate_model(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    all_labels = []\n    all_predictions = []\n    with torch.no_grad():\n        progress_bar = tqdm(val_loader, desc=\"Evaluating\", leave=False)\n        for hsi, text, labels in progress_bar:\n            hsi, labels = hsi.to(device), labels.to(device)\n            text = [t for t in text]\n            outputs = model(hsi, text[1])\n            labels = labels.float()\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            labels_new = torch.argmax(labels, dim=1)\n            correct += (predicted == labels_new).sum().item()\n            all_labels.extend(labels_new.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n            progress_bar.set_postfix({'val_loss': total_loss / (progress_bar.n + 1)})\n    accuracy = correct / total\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n\n    precision = precision_score(all_labels, all_predictions, average='weighted')\n    recall = recall_score(all_labels, all_predictions, average='weighted')\n\n    # Calculate confusion matrix\n    conf_matrix = confusion_matrix(all_labels, all_predictions)\n\n    # Calculate class-wise accuracies\n    class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n\n    return total_loss / len(val_loader), accuracy, f1, precision, recall, conf_matrix, class_accuracies, all_labels, all_predictions\n\ndef train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=25):\n    best_accuracy = 0\n    results = []\n    log_file = open(\"training_log.txt\", \"w\")\n\n    for epoch in range(num_epochs):\n        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n        val_loss, val_accuracy, val_f1, precision, recall, conf_matrix, class_accuracies, all_labels, all_predictions = evaluate_model(model, val_loader, criterion, device)\n        results.append({\n            'epoch': epoch + 1,\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n            'val_accuracy': val_accuracy,\n            'val_f1': val_f1,\n            'precision': precision,\n            'recall': recall,\n            'conf_matrix': conf_matrix,\n            'class_accuracies': class_accuracies,\n            'all_labels': all_labels,\n            'all_predictions': all_predictions\n        })\n\n        # Log and print the results\n        log_entry = (f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n                     f\"Val Accuracy: {val_accuracy:.4f}, Val F1 Score: {val_f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\\n\")\n        log_file.write(log_entry)\n        print(log_entry)\n\n        if val_accuracy > best_accuracy:\n            best_accuracy = val_accuracy\n            torch.save(model.state_dict(), 'best_model_.pth')\n\n    log_file.close()\n    return results\n\n# Train the model and get results\nresults = train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=25)\n\n# Extract the last epoch's results\nlast_epoch_results = results[-1]\nconf_matrix = last_epoch_results['conf_matrix']\nclass_accuracies = last_epoch_results['class_accuracies']\nall_labels = last_epoch_results['all_labels']\nall_predictions = last_epoch_results['all_predictions']\n\n# Calculate overall accuracy\noverall_accuracy = last_epoch_results['val_accuracy']\n\n# Calculate average accuracy\naverage_accuracy = np.mean(class_accuracies)\n\n# Calculate Kappa Coefficient\nkappa_score = cohen_kappa_score(all_labels, all_predictions)\n\n# Print the results\nprint(f\"Overall Accuracy: {overall_accuracy * 100:.4f}%\")\nprint(f\"Average Accuracy: {average_accuracy * 100:.4f}%\")\nprint(f\"Kappa Coefficient: {kappa_score * 100:.4f}\")\n\n# Print class-wise accuracies\nprint(\"Class-wise Accuracies:\")\nfor class_idx, accuracy in enumerate(class_accuracies, start=1):\n    print(f\"Class {class_idx}: {accuracy * 100:.4f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T01:06:07.071585Z","iopub.execute_input":"2025-02-16T01:06:07.071826Z","execution_failed":"2025-02-16T01:06:27.010Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c391c7446b7043f3ae0e7540e38b17aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30855365daff4f0fb0aa876dd5050df3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a7623e936804867a167ab24c16eff21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae2eb5f0a99490da063fb5ca14506c0"}},"metadata":{}},{"name":"stdout","text":"Data loaded.\nCSV file loaded.\nText data splitted based on spatial split.\nUsing patch size: 9\nPatching is done.\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  16%|█▌        | 371/2298 [00:06<00:33, 57.17it/s, val_loss=0.0587]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Prediction & Results","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/datapipeline\")\n\nimport PPreprocess\nfrom PPreprocess import DatasetPreprocess\nimport img as util\n\nimport numpy as np\n# Load the data\nfolder_name = '/kaggle/working/Indian Pines'\ndataset_preprocess = DatasetPreprocess(folder_name)\ndata, gt, captions = dataset_preprocess.load_data()\n\n# Split the data\ntrain_fraction = 0.10\nrem_classes = [0]  # Classes to exclude\n(train_rows, train_cols), (test_rows, test_cols) = util.data_split(gt, train_fraction=train_fraction, rem_classes=rem_classes)\n\n# Get the labels for training and testing sets\ntrain_labels = gt[train_rows, train_cols]\ntest_labels = gt[test_rows, test_cols]\n\n# Count the samples for each class\ntrain_counts = np.bincount(train_labels)\n\\\ntest_counts = np.bincount(test_labels)\n\n# Print class-wise sample counts for training\nprint(\"Class-wise sample counts for training:\")\nfor cls in np.unique(train_labels):\n    print(f\"Class {cls}: {train_counts[cls]} samples\")\n\n# Print class-wise sample counts for testing\nprint(\"\\nClass-wise sample counts for testing:\")\nfor cls in np.unique(test_labels):\n    print(f\"Class {cls}: {test_counts[cls]} samples\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-16T01:06:27.010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}